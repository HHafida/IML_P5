{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Installation de langdetect pour identifier les langues de notre df\n",
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVoNqjC4hE4G",
        "outputId": "c60e86cd-b9a2-4882-8d2b-41fdb02db5d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=d084bfdf28f33eebb077d92d847518e095baea2cb9fcf9ddf187a61e37a08825\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAev1OfXhRxl",
        "outputId": "87a567a0-998a-4708-f389-368aaff6f8b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.utils import resample\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "# Python libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import warnings\n",
        "\n",
        "# Ignorer tous les avertissements\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#python librairies\n",
        "\n",
        "from ast import literal_eval\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "import sklearn.metrics as metrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S5v04xMg5kY",
        "outputId": "2e0d5230-022e-49e6-a6dd-e0852c228e3b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path =\"/content/drive/MyDrive/\"\n",
        "df1 = pd.read_csv(path + 'QueryResults.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td5idks6g8vO",
        "outputId": "2d5608ba-6a83-4451-bc11-9353ae7aa0c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GlxsBK-Pg1Gd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Séléction des colonnes d'intérets\n",
        "df = df1[['Title', 'Body', 'Tags']]\n",
        "\n",
        "# Détection de la langue\n",
        "for index, row in df.iterrows():\n",
        "    text = row['Body']\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "        df.loc[index, 'lang'] = lang\n",
        "    except LangDetectException as e:\n",
        "        df.loc[index, 'lang'] = 'unknown'\n",
        "\n",
        "# Filtrage du df sur la langue la plus utilisée\n",
        "df = df[df['lang']=='en']\n",
        "df = df[['Body', 'Title','Tags']]\n",
        "\n",
        "# Nettoyage des balises HTML et code\n",
        "def remove_code(x):\n",
        "    soup = BeautifulSoup(x, \"lxml\")\n",
        "    code_to_remove = soup.findAll(\"code\")\n",
        "    for code in code_to_remove:\n",
        "        code.replace_with(\" \")\n",
        "    return str(soup)\n",
        "\n",
        "df['Body'] = df['Body'].apply(lambda text: remove_code(text))\n",
        "df.Body = [BeautifulSoup(text,\"lxml\").get_text() for text in df.Body]\n",
        "\n",
        "# Nettoyage du texte\n",
        "def clean_text(text):\n",
        "    if isinstance(text, list):\n",
        "        text = ' '.join(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"\\'\\w+\", ' ', text)\n",
        "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
        "    text = re.sub('[^\\\\w\\\\s#\\\\s++]', ' ', text)\n",
        "    text = re.sub(r'\\w*\\d+\\w*', ' ', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "df['Body'] = df['Body'].apply(lambda text: clean_text(text))\n",
        "df['Title'] = df['Title'].apply(lambda text: clean_text(text))\n",
        "\n",
        "# Nettoyage spécifique pour la colonne 'Tags'\n",
        "df['Tags'] = df['Tags'].apply(lambda text: re.sub(r'<|>', ',', text))\n",
        "df['Tags'] = df['Tags'].apply(lambda text: [tag for tag in text.split(',') if tag.strip()])\n",
        "df['Tags'] = df['Tags'].apply(lambda text: ' '.join(text) if isinstance(text, list) else text)\n",
        "\n",
        "# Tokenisation\n",
        "def words_tokenize(text):\n",
        "    text = text.split()\n",
        "    return text\n",
        "\n",
        "df['Body'] = df.Body.apply(lambda text: words_tokenize(text))\n",
        "df['Title'] = df.Title.apply(lambda text: words_tokenize(text))\n",
        "df['Tags'] = df.Tags.apply(lambda text: words_tokenize(text))\n",
        "\n",
        "# Suppression des stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['Body_filtered'] = df['Body'].apply(lambda x: ' '.join([word for word in x if word.lower() not in stop_words]))\n",
        "df['Title_filtered'] = df['Title'].apply(lambda x: ' '.join([word.lower() for word in x if word.lower() not in stop_words]))\n",
        "\n",
        "df = df.drop(columns=['Body', 'Title'])\n",
        "df = df.rename(columns={'Body_filtered': 'Body', 'Title_filtered': 'Title'})\n",
        "\n",
        "# Lemmatisation\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lem_word(text):\n",
        "    return [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in text]\n",
        "\n",
        "df['Body'] = df.Body.apply(lem_word)\n",
        "df['Title'] = df.Title.apply(lem_word)\n",
        "df['Tags'] = df.Tags.apply(lem_word)\n",
        "\n",
        "# Suppression des tags à occurrence unique\n",
        "def remove_single_occurrence_tags(dataframe, column):\n",
        "    all_tags = dataframe[column].explode()\n",
        "    tag_counts = all_tags.value_counts()\n",
        "    tags_to_remove = tag_counts[tag_counts == 1].index\n",
        "    filtered_dataframe = dataframe[~dataframe[column].apply(lambda x: any(tag in tags_to_remove for tag in x))]\n",
        "    return filtered_dataframe\n",
        "\n",
        "filtered_df = remove_single_occurrence_tags(df, 'Tags')\n",
        "df = filtered_df\n",
        "\n",
        "# Your preprocessing function\n",
        "def words_tokenize(text):\n",
        "    text = text.split()  # Split into words\n",
        "    return text\n",
        "\n",
        "# Apply tokenization to create the 'Text' column\n",
        "df['Text'] = df['Body'].apply(lambda x: ''.join(x)) + \" \" + df['Title'].apply(lambda x: ''.join(x))\n",
        "df['Text'] = df.Text.apply(lambda text: ''.join(words_tokenize(text)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importation du fichier test\n",
        "\n",
        "df ='/content/drive/MyDrive/df_cleaned.csv'\n",
        "df = pd.read_csv(df, index_col=False)\n",
        "\n",
        "# Your preprocessing function\n",
        "def words_tokenize(text):\n",
        "    text = text.split()  # Split into words\n",
        "    return text\n",
        "\n",
        "# Apply tokenization to create the 'Text' column\n",
        "df['Text'] = df['Body'].apply(lambda x: ''.join(x)) + \" \" + df['Title'].apply(lambda x: ''.join(x))\n",
        "df['Text'] = df.Text.apply(lambda text: ''.join(words_tokenize(text)))"
      ],
      "metadata": {
        "id": "hA7YRMwG6oMW"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import random\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import make_scorer, accuracy_score, recall_score, f1_score, average_precision_score, jaccard_score\n",
        "\n",
        "# Initialize the \"CountVectorizer\" TFIDF for text\n",
        "X = df[\"Text\"]\n",
        "vectorizer = TfidfVectorizer(analyzer=\"word\",\n",
        "                             max_df=0.97,\n",
        "                             min_df=3,\n",
        "                             tokenizer=None,\n",
        "                             preprocessor=''.join,\n",
        "                             stop_words=None,\n",
        "                             lowercase=False)\n",
        "\n",
        "# Fit and transform the data\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "\n",
        "# Multilabel binarizer for targets\n",
        "y = df['Tags']\n",
        "\n",
        "# Split tags and count their frequencies\n",
        "tag_counts = {}\n",
        "for tags in y:\n",
        "    for tag in tags.split():\n",
        "        tag_counts[tag] = tag_counts.get(tag, 0) + 1\n",
        "\n",
        "# Select the top 100 most frequent tags\n",
        "top_tags = sorted(tag_counts, key=tag_counts.get, reverse=True)[:100]\n",
        "\n",
        "# Filter and transform tags using the selected top tags\n",
        "tags_list_filtered = [[tag for tag in tags.split() if tag in top_tags] for tags in y]\n",
        "\n",
        "# Initialize and fit the multilabel binarizer\n",
        "multilabel_binarizer = MultiLabelBinarizer(classes=top_tags)\n",
        "y = multilabel_binarizer.fit_transform(tags_list_filtered)\n",
        "\n",
        "print(\"y shape : {}\".format(y.shape))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Initialize and train the SGDClassifier using OneVsRestClassifier\n",
        "sgd_classifier = OneVsRestClassifier(SGDClassifier())\n",
        "sgd_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels on the test set\n",
        "y_pred = sgd_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "jaccard = jaccard_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Jaccard Score:\", jaccard)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Recall Score:\", recall)\n",
        "print(\"Accuracy Score:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjW4mCd0x4w5",
        "outputId": "f8626148-5072-4a53-cf0b-0ee037b9af34"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y shape : (3453, 100)\n",
            "Jaccard Score: 0.284998831163367\n",
            "F1 Score: 0.4222138539923043\n",
            "Recall Score: 0.33410351201478744\n",
            "Accuracy Score: 0.13425925925925927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Chemin d'enregistrement du modèle\n",
        "path = \"/content/drive/MyDrive/\"\n",
        "model_filename = \"multilabel_binarizer.pkl\"\n",
        "model_path = os.path.join(path, model_filename)\n",
        "\n",
        "# Enregistrer le modèle multilabel binarizer dans un fichier\n",
        "joblib.dump(multilabel_binarizer, model_path)\n",
        "\n",
        "# Chemin d'enregistrement du modèle pour le TF-IDF vectorizer\n",
        "tfidf_vectorizer_filename = \"tfidf_vectorizer.pkl\"\n",
        "tfidf_vectorizer_path = os.path.join(path, tfidf_vectorizer_filename)\n",
        "\n",
        "# Enregistrer le TF-IDF vectorizer dans un fichier\n",
        "joblib.dump(vectorizer, tfidf_vectorizer_path)\n",
        "\n",
        "# Chemin d'enregistrement du modèle pour le SGD classifier\n",
        "sgd_classifier_filename = \"sgd_classifier.pkl\"\n",
        "sgd_classifier_path = os.path.join(path, sgd_classifier_filename)\n",
        "\n",
        "# Enregistrer le SGD classifier dans un fichier\n",
        "joblib.dump(sgd_classifier, sgd_classifier_path)\n",
        "\n",
        "# Chemin d'enregistrement du modèle pour X_tfidf\n",
        "x_tfidf_filename = \"X_tfidf.pkl\"\n",
        "x_tfidf_path = os.path.join(path, x_tfidf_filename)\n",
        "\n",
        "# Enregistrer X_tfidf dans un fichier\n",
        "joblib.dump(X_tfidf, x_tfidf_path)\n",
        "\n",
        "# Chemin d'enregistrement du modèle pour y_tag\n",
        "y_tag_filename = \"y_tag.pkl\"\n",
        "y_tag_path = os.path.join(path, y_tag_filename)\n",
        "\n",
        "# Enregistrer y_tag dans un fichier\n",
        "joblib.dump(y, y_tag_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lDZ8HmK7H8k",
        "outputId": "62d27e4c-4570-45c2-fbfb-3c0f5d487907"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/y_tag.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "id": "pz7iDJ3KmONN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85656eeb-c9b1-45b5-88dc-34e08b218891"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-2.6.0-py3-none-any.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.1)\n",
            "Collecting databricks-cli<1,>=0.8.7 (from mlflow)\n",
            "  Downloading databricks-cli-0.17.7.tar.gz (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4)\n",
            "Collecting gitpython<4,>=2.1.0 (from mlflow)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.0.1)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.20.3)\n",
            "Requirement already satisfied: pytz<2024 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2023.3)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.31.0)\n",
            "Requirement already satisfied: packaging<24 in /usr/local/lib/python3.10/dist-packages (from mlflow) (23.1)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.8.0)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4.4)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.11.3-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker<7,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.5)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.23.5)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.10.1)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.5.3)\n",
            "Collecting querystring-parser<2 (from mlflow)\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.20)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.2)\n",
            "Requirement already satisfied: pyarrow<13,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (9.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.4.4)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.1)\n",
            "Collecting gunicorn<22 (from mlflow)\n",
            "  Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.2)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.7.1)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\n",
            "Collecting urllib3<2.0.0,>=1.26.7 (from databricks-cli<1,>=0.8.7->mlflow)\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<7,>=4.0.0->mlflow) (1.6.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow) (2.3.7)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow) (2.1.2)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=2.1.0->mlflow)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow) (3.16.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.2.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (2.0.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: databricks-cli\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.17.7-py3-none-any.whl size=143855 sha256=4ccb60ebb452153a7c9cedd21badd54ce1f26a0557d67175a4d295922fb376f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/63/93/5402c1a09c1868a59d0b05013484e07af97a9d7b3dbd5bd39a\n",
            "Successfully built databricks-cli\n",
            "Installing collected packages: urllib3, smmap, querystring-parser, Mako, gunicorn, gitdb, alembic, gitpython, docker, databricks-cli, mlflow\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.3 databricks-cli-0.17.7 docker-6.1.3 gitdb-4.0.10 gitpython-3.1.32 gunicorn-21.2.0 mlflow-2.6.0 querystring-parser-1.2.4 smmap-5.0.0 urllib3-1.26.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pydantic==1.10.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFYaXxOZxSVw",
        "outputId": "0ffac907-2c38-44bb-fcdd-1a0f653c3b7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic==1.10.9\n",
            "  Downloading pydantic-1.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.9) (4.7.1)\n",
            "Installing collected packages: pydantic\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.2.1\n",
            "    Uninstalling pydantic-2.2.1:\n",
            "      Successfully uninstalled pydantic-2.2.1\n",
            "Successfully installed pydantic-1.10.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow"
      ],
      "metadata": {
        "id": "F3CGZcFCmqKG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from mlflow.models.signature import infer_signature\n",
        "\n",
        "# Assuming you have already trained the SGD classifier and named it 'sgd_classifier'\n",
        "final_model = sgd_classifier\n",
        "\n",
        "# Define and fit the multilabel binarizer\n",
        "multilabel_binarizer = MultiLabelBinarizer(classes=top_tags)\n",
        "y = multilabel_binarizer.fit_transform(tags_list_filtered)\n",
        "\n",
        "# Get the inferred signature for input and output data\n",
        "signature = infer_signature(X_train, y)  # Use y_bin instead of y_train\n",
        "\n",
        "# Chemin d'enregistrement du modèle avec le nom de dossier\n",
        "path = \"/content/drive/MyDrive/models\"\n",
        "model_folder = \"final_model\"\n",
        "\n",
        "# Chemin complet pour enregistrer le modèle\n",
        "model_path = os.path.join(path, model_folder)\n",
        "\n",
        "# Enregistrer le modèle avec la signature\n",
        "mlflow.sklearn.save_model(final_model, model_path, signature=signature)\n"
      ],
      "metadata": {
        "id": "O0Ljfus2ddq1"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}